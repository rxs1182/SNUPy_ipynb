\documentclass{article}
\usepackage{graphicx} % Required for inserting images

\title{Financial Sequence Classification using Transformer-Based Architectures}
\author{Rachel Tjarksen and Rohan Singh}
\date{July 2023}

\begin{document}

\maketitle

\begin{abstract}
placeholder text harry truman dorris day rwed china johnny ray south pacific walter winchell joe dimmagio joe mccarthy richard nixon studbaker television north korea south korea marilyn monroe
    
\end{abstract}

\section{Introduction}
NLP is taking over the world blah blah rise of transformers.  what led us? Money. contextful rep is better than simple vectorization <3 of text.  we used BERT because autoencoding is better than autoregressive.  Beacause we are analyzing text not generating it, so its better to take the whole sentence into account.  
\\\\
We did spacy and the other things because it was the best contextless analysis of text.  

\section{Problem}
Finding a suitable method for predictive analysis of financial performance of businesses.  

\section{Methods}
First we made a dataset then we used spacy to extract nouns adjectives and verbs then we trained tested split it.  then we created a pipeline which vectorizes the entire text corpora and then used logistic regression with the colors as labels.

\section{Data Set}
We used various news articles as our data set.  Title, Subheading, and color were extracted and labeled by us humans.  

\section{Model}

\section{Results}
Adjectives are very important and nouns are the least important.  however both of these things SUCK compared to the BERT architechture!!!!! we had a score of 0.5 for the adjectives, and 0.4 for the nouns and verbs, but 0.8 for the berthummel-bot.  

\section{Observations}

\section{References}

\end{document}
